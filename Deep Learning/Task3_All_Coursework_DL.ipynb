{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6621c91",
   "metadata": {},
   "source": [
    "# Task 3. Time Series\n",
    "\n",
    "### Student: Sandra M Nino A\n",
    "\n",
    "For this task, I conducted different approaches for each model. One major difficulty of this task is that each model took long time to run, so it was very hard to test and get to the best possible solution. \n",
    "\n",
    "Many of the experiments were based on predicting the 100 users, times or cluster names with a multioutput approach. This means, having the last dense layer of the network with 100 neurons. However, the sequence length had to be, at least, 100 to get some decent results. But, this takes a lot of time to run even with a simple model of 1 or 2 LSTM layers. Therefore, I tried decreasing the sequence length to predict the 100 \"at once\" but the R2 metric was negative. \n",
    "\n",
    "To simplify the task, the solution presented here is a one step forecast with univariate approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89613b0f",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219c302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6625fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf2282",
   "metadata": {},
   "source": [
    "Update the path to where the file is located to load the data. The file is called `UserLog.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2afd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Date_Time', 'Event_Type', 'Cluster_Name', 'Duration', 'Number_Users']\n",
    "data = pd.read_csv('./UserLog.csv',names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e094b50",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b52f9",
   "metadata": {},
   "source": [
    "This function creates the input sequences required for a time series task and its labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4ace24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size, future_target):\n",
    "  data_np = data.to_numpy()\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(data_np)-window_size-future_target+1):\n",
    "    row = [[a] for a in data_np[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = data_np[i+window_size]\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b2671",
   "metadata": {},
   "source": [
    "This function creates the model for regression tasks. These are for number of users and date time predictions. To simplify the task, it has 1 LSTM layer, 1 Dropout layer to avoid overfitting, and 1 output dense layer for the step prediction, in our case, one step forecast.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a6908f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_regression(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_shape)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd0006",
   "metadata": {},
   "source": [
    "This function creates the model for classification tasks. This is for the cluster names. The model consists in two LSTM layers, 3 Dropout layers, 2 dense layers, and one final dense layer to make the multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bfc41cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_classification(window_size, num_features, num_classes):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(LSTM(64, input_shape=(window_size, num_features), return_sequences=True))\n",
    "    model.add(LSTM(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa78d81",
   "metadata": {},
   "source": [
    "The following functions are used to calculate the metrics for the predictions and evaluate the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a310987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_metrics_regression(y_true, y_hat):\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "    \n",
    "    return mae, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c1fa4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_regression(model, X_test, y_test, scaler):\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Summary: Loss over the test dataset: %.2f, MAE: %.2f' % (score[0], score[1]))\n",
    "\n",
    "    y_true = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_hat = scaler.inverse_transform(model.predict(X_test).reshape(-1, 1))\n",
    "\n",
    "    mae, mse, r2 = calculate_metrics_regression(y_true, y_hat)\n",
    "\n",
    "    print('MAE:', mae)\n",
    "    print('MSE:', mse)\n",
    "    print('R2:', r2)\n",
    "\n",
    "    return mae, mse, r2\n",
    "\n",
    "def evaluate_model_classification(model, X_test, y_test, le):\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (score[0], score[1]))\n",
    "\n",
    "    y_true = le.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_hat = model.predict(X_test)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "    y_hat = le.inverse_transform(y_hat.reshape(-1, 1))\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_hat)\n",
    "    precision = precision_score(y_true, y_hat, average='macro')\n",
    "    recall = recall_score(y_true, y_hat, average='macro')\n",
    "    f1 = f1_score(y_true, y_hat, average='macro')\n",
    "\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1-Score:', f1)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fc31a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af638b",
   "metadata": {},
   "source": [
    "### For number of users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cb1be",
   "metadata": {},
   "source": [
    "This is a regression task, therefore, we scale the numbers to be in the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791c3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset = data['Number_Users'].to_numpy()\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "dataset = scaler.fit_transform(dataset.reshape(-1,1))\n",
    "\n",
    "data['Number_Users'] = dataset.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1297bd",
   "metadata": {},
   "source": [
    "### For cluster names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24a493",
   "metadata": {},
   "source": [
    "This is a classification task, therefore, we encode the cluster names in numbers as the neural network need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5869e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data['Cluster_Name'] = le.fit_transform(data['Cluster_Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33cc14",
   "metadata": {},
   "source": [
    "### For login/logout times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae120da",
   "metadata": {},
   "source": [
    "For this task, we convert the date time to a Unix Timestamp, which is a number. Therefore, this becomes a regression task. At the end, we scale the numbers to be in the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e1f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "bst_timezone = pytz.timezone('Europe/London')\n",
    "gmt_timezone = pytz.timezone('Etc/GMT')\n",
    "\n",
    "def convert_to_gmt(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%a %b %d %H:%M:%S %Z %Y')\n",
    "    \n",
    "    if date_obj.tzinfo == bst_timezone:\n",
    "        gmt_date_obj = date_obj.astimezone(gmt_timezone)\n",
    "        gmt_date_str = gmt_date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return gmt_date_str\n",
    "    else:\n",
    "        return date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data['Date_Format'] = data['Date_Time'].apply(convert_to_gmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d5e68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data['Date_Format']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c9f4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Event_Type</th>\n",
       "      <th>Cluster_Name</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Number_Users</th>\n",
       "      <th>Date_Format</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date_Format</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>9</td>\n",
       "      <td>1261840</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>10058927</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>28</td>\n",
       "      <td>6868990</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>2997017</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>8919800</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Date_Time Event_Type  Cluster_Name  \\\n",
       "Date_Format                                                                  \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN             9   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            28   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "\n",
       "                     Duration  Number_Users          Date_Format  \n",
       "Date_Format                                                       \n",
       "2010-01-01 00:00:00   1261840      0.000977  2010-01-01 00:00:00  \n",
       "2010-01-01 00:00:00  10058927      0.001953  2010-01-01 00:00:00  \n",
       "2010-01-01 00:00:00   6868990      0.002930  2010-01-01 00:00:00  \n",
       "2010-01-01 00:00:00   2997017      0.003906  2010-01-01 00:00:00  \n",
       "2010-01-01 00:00:00   8919800      0.004883  2010-01-01 00:00:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8526e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar, time; \n",
    "\n",
    "data['Date_Unix'] = data['Date_Format'].apply(lambda date_str : calendar.timegm(time.strptime(date_str, '%Y-%m-%d %H:%M:%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb016570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Event_Type</th>\n",
       "      <th>Cluster_Name</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Number_Users</th>\n",
       "      <th>Date_Format</th>\n",
       "      <th>Date_Unix</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date_Format</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>9</td>\n",
       "      <td>1261840</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>1262304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>10058927</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>1262304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>28</td>\n",
       "      <td>6868990</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>1262304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>2997017</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>1262304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>Fri Jan 01 00:00:00 GMT 2010</td>\n",
       "      <td>LOGIN</td>\n",
       "      <td>15</td>\n",
       "      <td>8919800</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>1262304000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Date_Time Event_Type  Cluster_Name  \\\n",
       "Date_Format                                                                  \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN             9   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            28   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "2010-01-01 00:00:00  Fri Jan 01 00:00:00 GMT 2010      LOGIN            15   \n",
       "\n",
       "                     Duration  Number_Users          Date_Format   Date_Unix  \n",
       "Date_Format                                                                   \n",
       "2010-01-01 00:00:00   1261840      0.000977  2010-01-01 00:00:00  1262304000  \n",
       "2010-01-01 00:00:00  10058927      0.001953  2010-01-01 00:00:00  1262304000  \n",
       "2010-01-01 00:00:00   6868990      0.002930  2010-01-01 00:00:00  1262304000  \n",
       "2010-01-01 00:00:00   2997017      0.003906  2010-01-01 00:00:00  1262304000  \n",
       "2010-01-01 00:00:00   8919800      0.004883  2010-01-01 00:00:00  1262304000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a2219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_date = MinMaxScaler()\n",
    "df = data['Date_Unix'].to_numpy().astype('float32')\n",
    "df = scaler_date.fit_transform(df.reshape(-1,1))\n",
    "\n",
    "data.loc[:, 'Date_Unix'] = df.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0207f3",
   "metadata": {},
   "source": [
    "## 3.1 Predict Number of Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835736f",
   "metadata": {},
   "source": [
    "For this task, we select input sequences of 50 and one future target. As mentioned at the beginning, I tried window sizes of 5, 10, 20, 50, 100, 120 to predict 100 number of users at once, but I didn't get good results and the training time takes long time which was impossible to manage due to the rest of the tasks. Therefore, I opted for this simpler solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2ff189",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "future_target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ff5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(data['Number_Users'], window_size, future_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c68959",
   "metadata": {},
   "source": [
    "We make our training, validation and test split like 70/10/20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "559cccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1721711, 50, 1)\n",
      "Shape of y_train: (1721711,)\n",
      "Shape of X_test: (491919, 50, 1)\n",
      "Shape of y_test: (491919,)\n",
      "Shape of X_val: (245958, 50, 1)\n",
      "Shape of y_val: (245958,)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f47d2",
   "metadata": {},
   "source": [
    "We build our regression model and use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06bf364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_users = build_model_regression((window_size, 1), future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b176f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_users = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d13e1a",
   "metadata": {},
   "source": [
    "We fit our model with 20 epochs. However, it reaches until the 8th epoch because it overfits very quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a14435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "53804/53804 [==============================] - 357s 7ms/step - loss: 0.0026 - mae: 0.0361 - val_loss: 1.9402e-04 - val_mae: 0.0128\n",
      "Epoch 2/20\n",
      "53804/53804 [==============================] - 352s 7ms/step - loss: 0.0025 - mae: 0.0351 - val_loss: 2.4346e-04 - val_mae: 0.0143\n",
      "Epoch 3/20\n",
      "53804/53804 [==============================] - 339s 6ms/step - loss: 0.0025 - mae: 0.0350 - val_loss: 4.6644e-05 - val_mae: 0.0059\n",
      "Epoch 4/20\n",
      "53804/53804 [==============================] - 340s 6ms/step - loss: 0.0024 - mae: 0.0349 - val_loss: 1.8839e-04 - val_mae: 0.0121\n",
      "Epoch 5/20\n",
      "53804/53804 [==============================] - 340s 6ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 1.4281e-04 - val_mae: 0.0100\n",
      "Epoch 6/20\n",
      "53804/53804 [==============================] - 341s 6ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 1.0074e-04 - val_mae: 0.0086\n",
      "Epoch 7/20\n",
      "53804/53804 [==============================] - 350s 6ms/step - loss: 0.0024 - mae: 0.0347 - val_loss: 9.3506e-05 - val_mae: 0.0084\n",
      "Epoch 8/20\n",
      "53796/53804 [============================>.] - ETA: 0s - loss: 0.0024 - mae: 0.0348Restoring model weights from the end of the best epoch: 3.\n",
      "53804/53804 [==============================] - 340s 6ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 1.1217e-04 - val_mae: 0.0094\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "history_users = model_users.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07430ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_users.save('./checkpoints/timeseries/ts-users-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39cb1a",
   "metadata": {},
   "source": [
    "After saving our model, we can run the following cell to load our final model. Update the path where the file is located. The file is called `ts-users-model-1.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "080ffd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = keras.models.load_model('./checkpoints/timeseries/ts-users-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732ec86",
   "metadata": {},
   "source": [
    "Now we can make some predictions. We see that the R2 is near 1. However, the values for MAE and MSE are quite high indicating low performance in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e51866de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15373/15373 [==============================] - 29s 2ms/step - loss: 6.8769e-05 - mae: 0.0072\n",
      "Summary: Loss over the test dataset: 0.00, MAE: 0.01\n",
      "15373/15373 [==============================] - 29s 2ms/step\n",
      "MAE: 7.400751\n",
      "MSE: 72.11\n",
      "R2: 0.9990146468169545\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_model_regression(final_model, X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31df2d",
   "metadata": {},
   "source": [
    "## 3.2 Predict Cluster Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4dca67",
   "metadata": {},
   "source": [
    "For this task, we select input sequences of 50 and one future target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4f9c7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(data['Cluster_Name'], 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b748797",
   "metadata": {},
   "source": [
    "We create our training, validation and test split like 70/10/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c443df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1721711, 50, 1)\n",
      "Shape of y_train: (1721711,)\n",
      "Shape of X_test: (491919, 50, 1)\n",
      "Shape of y_test: (491919,)\n",
      "Shape of X_val: (245958, 50, 1)\n",
      "Shape of y_val: (245958,)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5efc8",
   "metadata": {},
   "source": [
    "We build our regression model and use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b9384051",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(pd.unique(data['Cluster_Name']))\n",
    "model_clusters = build_model_classification(50, 1, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1274276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_clusters = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7b75b",
   "metadata": {},
   "source": [
    "For this task, different experiments did not get an accuracy greater than 0.13. Therefore, I reduced the number of epochs to 8 to show the training for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5b605e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "53804/53804 [==============================] - 702s 13ms/step - loss: 3.1896 - accuracy: 0.1280 - val_loss: 3.3010 - val_accuracy: 0.1132\n",
      "Epoch 2/8\n",
      "53804/53804 [==============================] - 717s 13ms/step - loss: 3.1486 - accuracy: 0.1327 - val_loss: 3.2922 - val_accuracy: 0.1145\n",
      "Epoch 3/8\n",
      "53804/53804 [==============================] - 750s 14ms/step - loss: 3.1366 - accuracy: 0.1337 - val_loss: 3.2813 - val_accuracy: 0.1142\n",
      "Epoch 4/8\n",
      "53804/53804 [==============================] - 752s 14ms/step - loss: 3.1318 - accuracy: 0.1340 - val_loss: 3.2832 - val_accuracy: 0.1134\n",
      "Epoch 5/8\n",
      "53804/53804 [==============================] - 720s 13ms/step - loss: 3.1341 - accuracy: 0.1338 - val_loss: 3.2993 - val_accuracy: 0.1133\n",
      "Epoch 6/8\n",
      "53800/53804 [============================>.] - ETA: 0s - loss: 3.1304 - accuracy: 0.1343Restoring model weights from the end of the best epoch: 3.\n",
      "53804/53804 [==============================] - 692s 13ms/step - loss: 3.1304 - accuracy: 0.1343 - val_loss: 3.3020 - val_accuracy: 0.1127\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "history_clusters = model_clusters.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10f6bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clusters.save('./checkpoints/timeseries/ts-clusters-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e1dc6",
   "metadata": {},
   "source": [
    "After saving our model, we can run the following cell to load our final model. Update the path where the file is located. The file is called `ts-clusters-model-1.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6d761567",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_clusters = keras.models.load_model('./checkpoints/timeseries/ts-clusters-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77931157",
   "metadata": {},
   "source": [
    "We can now evaluate our model for the classification task. We can see that the accuracy is 12%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86d1d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15373/15373 [==============================] - 52s 3ms/step - loss: 3.2866 - accuracy: 0.1163\n",
      "Summary: Loss over the test dataset: 3.29, Accuracy: 0.12\n",
      "   31/15373 [..............................] - ETA: 52s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandra.nino/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15373/15373 [==============================] - 52s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandra.nino/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sandra.nino/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11634639036101473\n",
      "Precision: 0.02542384284990699\n",
      "Recall: 0.03535080505250191\n",
      "F1-Score: 0.013102093308347336\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_model_classification(final_model_clusters, X_test, y_test, le) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923c71c",
   "metadata": {},
   "source": [
    "## 3.3 Predict Login/Logout times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f954b",
   "metadata": {},
   "source": [
    "For this task, we select input sequences of 50 and one future target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0c4224ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequences(data['Date_Unix'], 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a52cf",
   "metadata": {},
   "source": [
    "We create our training, validation and test sets like 70/10/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "013c4396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1721711, 50, 1)\n",
      "Shape of y_train: (1721711,)\n",
      "Shape of X_test: (491919, 50, 1)\n",
      "Shape of y_test: (491919,)\n",
      "Shape of X_val: (245958, 50, 1)\n",
      "Shape of y_val: (245958,)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.1)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aebe73",
   "metadata": {},
   "source": [
    "We build our regression model and use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "223f239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_time = build_model_regression((window_size, 1), future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d99466ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_time = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20a5ab",
   "metadata": {},
   "source": [
    "We fit our model with 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ef60b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53804/53804 [==============================] - 810s 15ms/step - loss: 0.0023 - mae: 0.0329 - val_loss: 0.0025 - val_mae: 0.0487\n",
      "Epoch 2/10\n",
      "53804/53804 [==============================] - 820s 15ms/step - loss: 0.0022 - mae: 0.0317 - val_loss: 0.0010 - val_mae: 0.0293\n",
      "Epoch 3/10\n",
      "53804/53804 [==============================] - 810s 15ms/step - loss: 0.0022 - mae: 0.0314 - val_loss: 0.0010 - val_mae: 0.0304\n",
      "Epoch 4/10\n",
      "53804/53804 [==============================] - 799s 15ms/step - loss: 0.0021 - mae: 0.0312 - val_loss: 0.0017 - val_mae: 0.0402\n",
      "Epoch 5/10\n",
      "53804/53804 [==============================] - 792s 15ms/step - loss: 0.0021 - mae: 0.0311 - val_loss: 0.0013 - val_mae: 0.0351\n",
      "Epoch 6/10\n",
      "53804/53804 [==============================] - 831s 15ms/step - loss: 0.0021 - mae: 0.0311 - val_loss: 9.3125e-04 - val_mae: 0.0289\n",
      "Epoch 7/10\n",
      "53804/53804 [==============================] - 815s 15ms/step - loss: 0.0021 - mae: 0.0310 - val_loss: 9.7623e-04 - val_mae: 0.0295\n",
      "Epoch 8/10\n",
      "53804/53804 [==============================] - 793s 15ms/step - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0010 - val_mae: 0.0311\n",
      "Epoch 9/10\n",
      "53804/53804 [==============================] - 807s 15ms/step - loss: 0.0021 - mae: 0.0309 - val_loss: 5.0130e-04 - val_mae: 0.0205\n",
      "Epoch 10/10\n",
      "53804/53804 [==============================] - 804s 15ms/step - loss: 0.0021 - mae: 0.0308 - val_loss: 0.0013 - val_mae: 0.0346\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history_times = model_time.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "315e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_time.save('./checkpoints/timeseries/ts-login-logout-time-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9dd18",
   "metadata": {},
   "source": [
    "After saving our model, we can run the following cell to load our final model. Update the path where the file is located. The file is called `ts-login-logout-time-model-1.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8bd55a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_time = keras.models.load_model('./checkpoints/timeseries/ts-login-logout-time-model-1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaa42f",
   "metadata": {},
   "source": [
    "Now we can make some predictions. The very high MAE, MSE, and negative R2 indicate that the model is performing poorly and is not able to effectively predict the timestamp for login/logout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "33149796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15373/15373 [==============================] - 58s 4ms/step - loss: 0.0105 - mae: 0.0984\n",
      "Summary: Loss over the test dataset: 0.01, MAE: 0.10\n",
      "15373/15373 [==============================] - 59s 4ms/step\n",
      "MAE: 3101496.2\n",
      "MSE: 10483952000000.0\n",
      "R2: -8.347839716688487\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_model_regression(final_model_time, X_test, y_test, scaler_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1af03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
